# Toy Example


https://github.com/mustafaaljadery/gym-play/assets/68826650/2db9a6c6-de48-4777-86e2-58aff5fa3e9d


- The Q learning algorithm memorizes the path vs the the actual reward chest.
- It's very simple, and everyone understand the reason for misalignment here - the training environment is not diverse.
- The "memorizes" the path rather than the reward, which is misalignment. That's what I was trying to showcase here. 
- It's still very interesting to write code that showcases concepts.

TODO:

- How can I build a toy example of deceptive alignment?
