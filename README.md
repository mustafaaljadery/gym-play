# Toy Example

- The Q learning algorithm memorizes the path vs the the actual reward chest.
- It's very simple, and everyone understand the reason for misalignment here - the training environment is not diverse.
- It's still very interesting to write code that showcases concepts.

TODO:

- How can I build a toy example of deceptive alignment?
